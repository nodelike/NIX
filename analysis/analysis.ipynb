{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_split(data_length, n_splits=30, test_size=5):\n",
    "    \"\"\"Monte Carlo Time Series Validation.\"\"\"\n",
    "    indices = np.arange(data_length)\n",
    "    splits = []\n",
    "    for _ in range(n_splits):\n",
    "        if len(indices) <= test_size:\n",
    "            return [(indices, indices)]\n",
    "        start = np.random.randint(0, len(indices)-test_size)\n",
    "        splits.append((indices[:start], indices[start:start+test_size]))\n",
    "    return splits\n",
    "\n",
    "def calculate_position_size(probability):\n",
    "    \"\"\"Calculate position size based on prediction confidence.\"\"\"\n",
    "    risk = 0.02  # 2% per trade\n",
    "    confidence = abs(probability - 0.5) * 2\n",
    "    return risk * confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 15:49:49 - INFO - Loading and preparing data...\n",
      "2025-02-23 15:49:49 - INFO - Total trading days in data: 18\n",
      "2025-02-23 15:49:49 - WARNING - WARNING: Dataset contains less than 100 trading days. Results may be unreliable.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Loading and preparing data...\")\n",
    "# Read the data\n",
    "nifty_df = pd.read_csv('nifty_1min_data.csv')\n",
    "vix_df = pd.read_csv('vix_1min_data.csv')\n",
    "\n",
    "# Data Preparation & Cleaning\n",
    "for df in [nifty_df, vix_df]:\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df.set_index('Datetime', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "# Filter market hours only (9:15 AM to 3:30 PM IST)\n",
    "nifty_df = nifty_df.between_time('09:15', '15:30')\n",
    "vix_df = vix_df.between_time('09:15', '15:30')\n",
    "\n",
    "# Get unique dates using pandas datetime index\n",
    "unique_dates = pd.Series(nifty_df.index.date).nunique()\n",
    "logger.info(f\"Total trading days in data: {unique_dates}\")\n",
    "\n",
    "if unique_dates < 100:\n",
    "    logger.warning(\"WARNING: Dataset contains less than 100 trading days. Results may be unreliable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_5min_features(df):\n",
    "    \"\"\"Extract features from first 5 minutes and previous day's last 30 minutes.\"\"\"\n",
    "    def process_group(group):\n",
    "        if len(group) >= 5:\n",
    "            first_5_min = group.iloc[:5]\n",
    "            \n",
    "            # Get previous day's data\n",
    "            prev_day = group.index[0].date() - pd.Timedelta(days=1)\n",
    "            prev_day_data = df[df.index.date == prev_day].between_time('15:00', '15:30')\n",
    "            \n",
    "            features = {\n",
    "                # Current day first 5 min features\n",
    "                'vix_open': first_5_min['Open'].iloc[0],\n",
    "                'vix_high': first_5_min['High'].max(),\n",
    "                'vix_low': first_5_min['Low'].min(),\n",
    "                'vix_close': first_5_min['Close'].iloc[-1],\n",
    "                'vix_volatility': first_5_min['Close'].std(),\n",
    "                'vix_pct_change': (first_5_min['Close'].iloc[-1] / first_5_min['Close'].iloc[0] - 1) * 100,\n",
    "                'vix_range': (first_5_min['High'].max() - first_5_min['Low'].min()) / first_5_min['Open'].iloc[0] * 100,\n",
    "                'vix_oc_ratio': (first_5_min['Close'].iloc[-1] - first_5_min['Open'].iloc[0]) / first_5_min['Open'].iloc[0]\n",
    "            }\n",
    "            \n",
    "            # Add previous day's last 30 min features if available\n",
    "            if len(prev_day_data) >= 25:  # At least 25 minutes of data\n",
    "                features.update({\n",
    "                    'prev_day_close': prev_day_data['Close'].iloc[-1],\n",
    "                    'prev_day_vwap': (prev_day_data['Close'] * prev_day_data['Volume']).sum() / prev_day_data['Volume'].sum(),\n",
    "                    'prev_day_volatility': prev_day_data['Close'].std(),\n",
    "                    'prev_day_trend': (prev_day_data['Close'].iloc[-1] / prev_day_data['Close'].iloc[0] - 1) * 100,\n",
    "                    'prev_day_high': prev_day_data['High'].max(),\n",
    "                    'prev_day_low': prev_day_data['Low'].min(),\n",
    "                    'prev_day_range': (prev_day_data['High'].max() - prev_day_data['Low'].min()) / prev_day_data['Open'].iloc[0] * 100,\n",
    "                    'gap_pct': (first_5_min['Open'].iloc[0] / prev_day_data['Close'].iloc[-1] - 1) * 100\n",
    "                })\n",
    "            else:\n",
    "                # If previous day data not available, use neutral values\n",
    "                features.update({\n",
    "                    'prev_day_close': first_5_min['Open'].iloc[0],  # Use current day open\n",
    "                    'prev_day_vwap': first_5_min['Open'].iloc[0],\n",
    "                    'prev_day_volatility': 0,\n",
    "                    'prev_day_trend': 0,\n",
    "                    'prev_day_high': first_5_min['Open'].iloc[0],\n",
    "                    'prev_day_low': first_5_min['Open'].iloc[0],\n",
    "                    'prev_day_range': 0,\n",
    "                    'gap_pct': 0\n",
    "                })\n",
    "            \n",
    "            return pd.Series(features)\n",
    "        return None\n",
    "    \n",
    "    return df.groupby(pd.Grouper(freq='D')).apply(process_group).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nifty_target(df):\n",
    "    \"\"\"Calculate 3-hour return for NIFTY.\"\"\"\n",
    "    def calculate_return(group):\n",
    "        if len(group) >= 180:  # 3 hours = 180 minutes\n",
    "            start_price = group['Open'].iloc[0]\n",
    "            end_price = group['Close'].iloc[179]  # 180th minute\n",
    "            return (end_price / start_price - 1) * 100\n",
    "        return None\n",
    "    \n",
    "    return df.groupby(pd.Grouper(freq='D')).apply(calculate_return).dropna().to_frame('nifty_3hr_pct_change')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 15:49:49 - INFO - Generating features...\n",
      "/var/folders/qq/3d_c2cq958v9kssh_88364qc0000gn/T/ipykernel_21959/430436298.py:27: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  'prev_day_vwap': (prev_day_data['Close'] * prev_day_data['Volume']).sum() / prev_day_data['Volume'].sum(),\n",
      "2025-02-23 15:49:49 - INFO - Generated features for 4 trading days\n",
      "2025-02-23 15:49:49 - INFO - Generated targets for 18 trading days\n",
      "2025-02-23 15:49:49 - INFO - Final dataset size: 4 trading days\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Insufficient data: Need at least 10 trading days",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal dataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(merged_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m trading days\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(merged_df) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInsufficient data: Need at least 10 trading days\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Create binary trend target (1 for positive returns, 0 for negative)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrend\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnifty_3hr_pct_change\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Insufficient data: Need at least 10 trading days"
     ]
    }
   ],
   "source": [
    "logger.info(\"Generating features...\")\n",
    "# Generate VIX features\n",
    "vix_features = get_first_5min_features(vix_df)\n",
    "logger.info(f\"Generated features for {len(vix_features)} trading days\")\n",
    "\n",
    "# Generate NIFTY target\n",
    "nifty_target = get_nifty_target(nifty_df)\n",
    "logger.info(f\"Generated targets for {len(nifty_target)} trading days\")\n",
    "\n",
    "# Ensure data alignment\n",
    "common_dates = vix_features.index.intersection(nifty_target.index)\n",
    "vix_features = vix_features.loc[common_dates]\n",
    "nifty_target = nifty_target.loc[common_dates]\n",
    "\n",
    "# Merge features and target\n",
    "merged_df = pd.concat([vix_features, nifty_target], axis=1).dropna()\n",
    "logger.info(f\"Final dataset size: {len(merged_df)} trading days\")\n",
    "\n",
    "if len(merged_df) < 10:\n",
    "    raise ValueError(\"Insufficient data: Need at least 10 trading days\")\n",
    "\n",
    "# Create binary trend target (1 for positive returns, 0 for negative)\n",
    "merged_df['trend'] = (merged_df['nifty_3hr_pct_change'] > 0).astype(int)\n",
    "class_balance = merged_df['trend'].mean() * 100\n",
    "logger.info(f\"Positive trend days: {merged_df['trend'].sum()} ({class_balance:.1f}%)\")\n",
    "\n",
    "if not (40 <= class_balance <= 60):\n",
    "    logger.warning(f\"WARNING: Significant class imbalance detected ({class_balance:.1f}% positive)\")\n",
    "\n",
    "logger.info(\"Performing feature engineering...\")\n",
    "# Additional Feature Engineering\n",
    "merged_df['vix_nifty_correlation'] = merged_df['vix_pct_change'].rolling(3, min_periods=1).corr(merged_df['nifty_3hr_pct_change'])\n",
    "merged_df['vix_acceleration'] = merged_df['vix_pct_change'].diff()\n",
    "merged_df['vol_ratio'] = merged_df['vix_volatility'] / merged_df['vix_volatility'].rolling(2, min_periods=1).mean()\n",
    "merged_df['day_of_week'] = merged_df.index.dayofweek\n",
    "\n",
    "# Add moving averages with smaller windows\n",
    "merged_df['vix_ma3'] = merged_df['vix_close'].rolling(3, min_periods=1).mean()\n",
    "merged_df['vix_ma5'] = merged_df['vix_close'].rolling(5, min_periods=1).mean()\n",
    "\n",
    "# Fill NaN values with appropriate methods\n",
    "for col in merged_df.columns:\n",
    "    if col not in ['trend', 'nifty_3hr_pct_change']:\n",
    "        if 'ratio' in col or 'correlation' in col:\n",
    "            merged_df[col] = merged_df[col].fillna(0)\n",
    "        else:\n",
    "            merged_df[col] = merged_df[col].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "logger.info(f\"Dataset size after feature engineering: {len(merged_df)} trading days\")\n",
    "\n",
    "logger.info(\"Analyzing feature correlations...\")\n",
    "# Visual Analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "corr_matrix = merged_df.corr()\n",
    "sns.heatmap(corr_matrix[['nifty_3hr_pct_change']], annot=True, fmt='.2f')\n",
    "plt.title('Feature Correlation with Target')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png')\n",
    "plt.close()\n",
    "\n",
    "# Display correlation results\n",
    "logger.info(\"\\nFeature Correlations with NIFTY 3-hour Returns:\")\n",
    "correlations = corr_matrix['nifty_3hr_pct_change'].sort_values(ascending=False)\n",
    "for feature, corr in correlations.items():\n",
    "    if feature not in ['nifty_3hr_pct_change', 'trend']:\n",
    "        logger.info(f\"{feature:20} : {corr:+.3f}\")\n",
    "\n",
    "logger.info(\"\\nGenerating pair plots...\")\n",
    "# Select most important features for pair plot\n",
    "important_features = ['vix_pct_change', 'vix_volatility', 'vix_range', 'nifty_3hr_pct_change']\n",
    "sns.pairplot(merged_df[important_features], diag_kind='kde')\n",
    "plt.savefig('pair_plots.png')\n",
    "plt.close()\n",
    "\n",
    "logger.info(\"Building and evaluating model...\")\n",
    "# Model Building - Explicitly exclude target variables from features\n",
    "features = [col for col in merged_df.columns if col not in ['nifty_3hr_pct_change', 'trend']]\n",
    "X = merged_df[features].values  # Convert to numpy array\n",
    "y = merged_df['trend'].values   # Convert to numpy array\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "if len(X) >= 10:\n",
    "    logger.info(\"Applying SMOTE to handle class imbalance...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "else:\n",
    "    logger.warning(\"Dataset too small for SMOTE. Using original data.\")\n",
    "    X_resampled, y_resampled = X, y\n",
    "\n",
    "# Use Monte Carlo cross-validation for small datasets\n",
    "n_splits = max(1, len(merged_df) // 5)  # 5 samples per test set\n",
    "splits = monte_carlo_split(len(X_resampled), n_splits=min(30, n_splits), test_size=5)\n",
    "\n",
    "model = GradientBoostingClassifier(\n",
    "    loss='log_loss',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Model Evaluation\n",
    "logger.info(\"\\nModel Performance Across Monte Carlo Splits:\")\n",
    "logger.info(\"-\" * 50)\n",
    "fold_accuracies = []\n",
    "fold_probas = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(splits, 1):\n",
    "    X_train, X_test = X_resampled[train_index], X_resampled[test_index]\n",
    "    y_train, y_test = y_resampled[train_index], y_resampled[test_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    probas = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    fold_accuracies.append(accuracy)\n",
    "    fold_probas.extend(probas)\n",
    "    \n",
    "    logger.info(f\"\\nFold {fold}:\")\n",
    "    logger.info(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "    logger.info(f\"Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    report = classification_report(y_test, model.predict(X_test), zero_division=0)\n",
    "    logger.info(\"\\nClassification Report:\")\n",
    "    logger.info(report)\n",
    "\n",
    "logger.info(f\"\\nAverage Model Accuracy: {np.mean(fold_accuracies):.2f}\")\n",
    "logger.info(f\"Std Dev of Accuracy: {np.std(fold_accuracies):.2f}\")\n",
    "logger.info(f\"Average Prediction Confidence: {np.mean(np.abs(np.array(fold_probas) - 0.5)) * 2:.2f}\")\n",
    "\n",
    "logger.info(\"\\nAnalyzing feature importance...\")\n",
    "# Feature Importance Analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "importance = pd.Series(model.feature_importances_, index=features)\n",
    "importance_sorted = importance.sort_values(ascending=True)\n",
    "importance_sorted.plot(kind='barh')\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# Display feature importance\n",
    "logger.info(\"\\nTop 10 Most Important Features:\")\n",
    "for feature, importance in importance_sorted.nlargest(10).items():\n",
    "    logger.info(f\"{feature:20} : {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(vix_data, prev_day_data=None):\n",
    "    \"\"\"\n",
    "    Create features from live VIX data for prediction.\n",
    "    \n",
    "    Args:\n",
    "        vix_data (pd.DataFrame): Live VIX data with OHLCV prices for first 5 minutes\n",
    "        prev_day_data (pd.DataFrame): Previous day's last 30 minutes of data (optional)\n",
    "        \n",
    "    Returns:\n",
    "        list: Feature vector for prediction\n",
    "    \"\"\"\n",
    "    if len(vix_data) < 5:\n",
    "        raise ValueError(\"Need at least 5 minutes of VIX data\")\n",
    "        \n",
    "    first_5min = vix_data.iloc[:5]\n",
    "    features = {\n",
    "        'vix_open': first_5min['Open'].iloc[0],\n",
    "        'vix_high': first_5min['High'].max(),\n",
    "        'vix_low': first_5min['Low'].min(),\n",
    "        'vix_close': first_5min['Close'].iloc[-1],\n",
    "        'vix_volatility': first_5min['Close'].std(),\n",
    "        'vix_pct_change': (first_5min['Close'].iloc[-1] / first_5min['Close'].iloc[0] - 1) * 100,\n",
    "        'vix_range': (first_5min['High'].max() - first_5min['Low'].min()) / first_5min['Open'].iloc[0] * 100,\n",
    "        'vix_oc_ratio': (first_5min['Close'].iloc[-1] - first_5min['Open'].iloc[0]) / first_5min['Open'].iloc[0],\n",
    "        'vix_acceleration': 0,\n",
    "        'vol_ratio': 1,\n",
    "        'day_of_week': vix_data.index[0].dayofweek,\n",
    "        'vix_ma3': first_5min['Close'].mean(),\n",
    "        'vix_ma5': first_5min['Close'].mean(),\n",
    "        'vix_nifty_correlation': 0\n",
    "    }\n",
    "    \n",
    "    # Add previous day features if available\n",
    "    if prev_day_data is not None and len(prev_day_data) >= 25:\n",
    "        features.update({\n",
    "            'prev_day_close': prev_day_data['Close'].iloc[-1],\n",
    "            'prev_day_vwap': (prev_day_data['Close'] * prev_day_data['Volume']).sum() / prev_day_data['Volume'].sum(),\n",
    "            'prev_day_volatility': prev_day_data['Close'].std(),\n",
    "            'prev_day_trend': (prev_day_data['Close'].iloc[-1] / prev_day_data['Close'].iloc[0] - 1) * 100,\n",
    "            'prev_day_high': prev_day_data['High'].max(),\n",
    "            'prev_day_low': prev_day_data['Low'].min(),\n",
    "            'prev_day_range': (prev_day_data['High'].max() - prev_day_data['Low'].min()) / prev_day_data['Open'].iloc[0] * 100,\n",
    "            'gap_pct': (first_5min['Open'].iloc[0] / prev_day_data['Close'].iloc[-1] - 1) * 100\n",
    "        })\n",
    "    else:\n",
    "        features.update({\n",
    "            'prev_day_close': first_5min['Open'].iloc[0],\n",
    "            'prev_day_vwap': first_5min['Open'].iloc[0],\n",
    "            'prev_day_volatility': 0,\n",
    "            'prev_day_trend': 0,\n",
    "            'prev_day_high': first_5min['Open'].iloc[0],\n",
    "            'prev_day_low': first_5min['Open'].iloc[0],\n",
    "            'prev_day_range': 0,\n",
    "            'gap_pct': 0\n",
    "        })\n",
    "    \n",
    "    # Convert to list in the same order as training features\n",
    "    return [features[feature] for feature in features]\n",
    "\n",
    "def daily_prediction(live_vix, prev_day_data=None):\n",
    "    \"\"\"\n",
    "    Make predictions using live VIX data.\n",
    "    \n",
    "    Args:\n",
    "        live_vix (pd.DataFrame): First 5 minutes of live VIX data\n",
    "        prev_day_data (pd.DataFrame): Previous day's last 30 minutes of data (optional)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (signal, probability, position_size)\n",
    "    \"\"\"\n",
    "    features = create_features(live_vix, prev_day_data)\n",
    "    proba = model.predict_proba([features])[0][1]\n",
    "    position_size = calculate_position_size(proba)\n",
    "    \n",
    "    if proba > 0.65:\n",
    "        signal = \"STRONG BUY\"\n",
    "    elif proba > 0.55:\n",
    "        signal = \"BUY\"\n",
    "    elif proba < 0.35:\n",
    "        signal = \"STRONG SELL\"\n",
    "    else:\n",
    "        signal = \"NEUTRAL\"\n",
    "        \n",
    "    return signal, proba, position_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 15:49:51 - INFO - \n",
      "Analysis complete! The following files have been generated:\n",
      "2025-02-23 15:49:51 - INFO - 1. correlation_heatmap.png - Shows feature correlations with target\n",
      "2025-02-23 15:49:51 - INFO - 2. pair_plots.png - Visualizes relationships between key variables\n",
      "2025-02-23 15:49:51 - INFO - 3. feature_importance.png - Shows relative importance of each feature\n",
      "2025-02-23 15:49:51 - INFO - 4. vix_patterns.png - Displays discovered VIX patterns\n",
      "2025-02-23 15:49:51 - WARNING - \n",
      "WARNING: Current dataset is too small for reliable predictions.\n",
      "2025-02-23 15:49:51 - WARNING - Recommended: Collect at least 6 months of historical data.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fold_accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWARNING: Current dataset is too small for reliable predictions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended: Collect at least 6 months of historical data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstd(fold_accuracies) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.15\u001b[39m:\n\u001b[1;32m     12\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWARNING: High variance in model performance detected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended: Use more conservative position sizing.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fold_accuracies' is not defined"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\nAnalysis complete! The following files have been generated:\")\n",
    "logger.info(\"1. correlation_heatmap.png - Shows feature correlations with target\")\n",
    "logger.info(\"2. pair_plots.png - Visualizes relationships between key variables\")\n",
    "logger.info(\"3. feature_importance.png - Shows relative importance of each feature\")\n",
    "logger.info(\"4. vix_patterns.png - Displays discovered VIX patterns\")\n",
    "\n",
    "# Final warnings and recommendations\n",
    "if len(merged_df) < 100:\n",
    "    logger.warning(\"\\nWARNING: Current dataset is too small for reliable predictions.\")\n",
    "    logger.warning(\"Recommended: Collect at least 6 months of historical data.\")\n",
    "if np.std(fold_accuracies) > 0.15:\n",
    "    logger.warning(\"\\nWARNING: High variance in model performance detected.\")\n",
    "    logger.warning(\"Recommended: Use more conservative position sizing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
